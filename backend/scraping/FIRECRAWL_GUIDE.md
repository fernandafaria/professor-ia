# Guia de Uso do Firecrawl para Web Scraping

Este guia explica como usar o Firecrawl para fazer web scraping das fontes mapeadas no documento `mapeamento_webscraping_edtech.md`.

## üìã √çndice

1. [Configura√ß√£o](#configura√ß√£o)
2. [Uso B√°sico](#uso-b√°sico)
3. [Integra√ß√£o com Pipeline](#integra√ß√£o-com-pipeline)
4. [Fontes Priorit√°rias](#fontes-priorit√°rias)
5. [Exemplos Pr√°ticos](#exemplos-pr√°ticos)

---

## üîß Configura√ß√£o

### 1. Instalar Depend√™ncias

```bash
# Instalar biblioteca Firecrawl
pip install firecrawl-py

# Ou via requirements.txt
pip install -r backend/requirements.txt
```

### 2. Configurar API Key

O Firecrawl j√° est√° configurado no MCP com a API key:
```
fc-d9e38b1898aa4067be99276054db16be
```

Para usar programaticamente, configure a vari√°vel de ambiente:

```bash
# Linux/Mac
export FIRECRAWL_API_KEY='fc-d9e38b1898aa4067be99276054db16be'

# Windows
set FIRECRAWL_API_KEY=fc-d9e38b1898aa4067be99276054db16be
```

Ou adicione ao arquivo `.env`:

```env
FIRECRAWL_API_KEY=fc-d9e38b1898aa4067be99276054db16be
```

---

## üöÄ Uso B√°sico

### Scraping de URL √önica

```python
from backend.scraping.scrapers.firecrawl import FirecrawlScraper

source_config = {
    "name": "Nova Escola",
    "url": "https://novaescola.org.br/conteudo/12345/plano-de-aula",
    "type": "educational",
    "priority": "high",
}

scraper = FirecrawlScraper(source_config)
documents = scraper.scrape()

for doc in documents:
    print(f"T√≠tulo: {doc['title']}")
    print(f"Conte√∫do: {doc['content'][:200]}...")
```

### Crawling de Site Inteiro

```python
scraper = FirecrawlScraper(source_config)

# Coletar at√© 10 p√°ginas do site
documents = scraper.scrape(crawl=True, max_pages=10)
```

### Lista de URLs Espec√≠ficas

```python
urls = [
    "https://novaescola.org.br/conteudo/12345/plano-de-aula",
    "https://novaescola.org.br/conteudo/12346/plano-de-aula",
    "https://novaescola.org.br/conteudo/12347/plano-de-aula",
]

documents = scraper.scrape(urls=urls)
```

---

## üîÑ Integra√ß√£o com Pipeline

### Usar Firecrawl atrav√©s do Pipeline

```python
from backend.scraping.pipeline import ScrapingPipeline

pipeline = ScrapingPipeline()

# Fazer scraping usando Firecrawl
documents = pipeline.scrape_source(
    "Nova Escola",
    use_firecrawl=True,  # Usar Firecrawl em vez de scraper tradicional
    crawl=True,
    max_pages=5
)

# Adicionar ao RAG
pipeline.add_to_rag(documents)
```

### Fontes que Usam Firecrawl por Padr√£o

As seguintes fontes usam Firecrawl automaticamente:
- Nova Escola
- Projeto √Ågatha Edu
- Fandom Wikis
- Liquipedia
- Globo Esporte
- ESPN Brasil
- Letras.mus.br

---

## üéØ Fontes Priorit√°rias

### Fase 1 - MVP (Prioridade Cr√≠tica/Alta)

#### 1. API BNCC Cientificar
**Nota:** Esta √© uma API REST, n√£o precisa de scraping. Use o `BNCCAPIScraper`.

#### 2. Projeto √Ågatha Edu
```python
source_config = {
    "name": "Projeto √Ågatha Edu",
    "url": "https://www.projetoagathaedu.com.br",
    "type": "questions",
    "priority": "very_high",
}

scraper = FirecrawlScraper(source_config)
documents = scraper.scrape(crawl=True, max_pages=50)
```

#### 3. Nova Escola
```python
source_config = {
    "name": "Nova Escola",
    "url": "https://novaescola.org.br",
    "type": "educational",
    "priority": "high",
}

scraper = FirecrawlScraper(source_config)

# Coletar planos de aula
documents = scraper.scrape_article_list(
    list_url="https://novaescola.org.br/conteudo",
    article_selector="a[href*='/conteudo/']",
    max_articles=100
)
```

### Fase 2 - Expans√£o

#### Fontes Culturais (Games, Futebol, M√∫sica)

```python
# Games - Fandom Wikis
games_config = {
    "name": "Fandom Wikis",
    "url": "https://www.fandom.com",
    "type": "wiki",
    "priority": "high",
}

# Futebol - Globo Esporte
futebol_config = {
    "name": "Globo Esporte",
    "url": "https://ge.globo.com",
    "type": "news",
    "priority": "medium",
}

# M√∫sica - Letras.mus.br
musica_config = {
    "name": "Letras.mus.br",
    "url": "https://www.letras.mus.br",
    "type": "lyrics",
    "priority": "medium",
}
```

---

## üí° Exemplos Pr√°ticos

### Exemplo 1: Coletar Planos de Aula da Nova Escola

```python
from backend.scraping.scrapers.firecrawl import FirecrawlScraper
from backend.scraping.pipeline import ScrapingPipeline

# Configura√ß√£o
source_config = {
    "name": "Nova Escola",
    "url": "https://novaescola.org.br",
    "type": "educational",
    "priority": "high",
}

# Criar scraper
scraper = FirecrawlScraper(source_config)

# Coletar artigos de uma p√°gina de listagem
documents = scraper.scrape_article_list(
    list_url="https://novaescola.org.br/conteudo",
    article_selector="a[href*='/conteudo/']",
    max_articles=50
)

print(f"Coletados {len(documents)} planos de aula")

# Adicionar ao RAG
pipeline = ScrapingPipeline()
pipeline.add_to_rag(documents)
```

### Exemplo 2: Coletar Quest√µes do Projeto √Ågatha

```python
source_config = {
    "name": "Projeto √Ågatha Edu",
    "url": "https://www.projetoagathaedu.com.br",
    "type": "questions",
    "priority": "very_high",
}

scraper = FirecrawlScraper(source_config)

# Fazer crawling do site
documents = scraper.scrape(crawl=True, max_pages=100)

# Filtrar apenas quest√µes
questions = [
    doc for doc in documents 
    if "quest√£o" in doc["content"].lower() or "enem" in doc["content"].lower()
]

print(f"Coletadas {len(questions)} quest√µes")
```

### Exemplo 3: Pipeline Completo com Firecrawl

```python
from backend.scraping.pipeline import ScrapingPipeline

pipeline = ScrapingPipeline()

# Fontes priorit√°rias
priority_sources = [
    "Nova Escola",
    "Projeto √Ågatha Edu",
]

all_documents = []

for source_name in priority_sources:
    print(f"Processando: {source_name}")
    
    # Usar Firecrawl para estas fontes
    docs = pipeline.scrape_source(
        source_name,
        use_firecrawl=True,
        crawl=True,
        max_pages=10
    )
    
    all_documents.extend(docs)
    print(f"  ‚úì {len(docs)} documentos coletados")

# Adicionar tudo ao RAG
pipeline.add_to_rag(all_documents)

print(f"\nTotal: {len(all_documents)} documentos adicionados ao RAG")
```

---

## üìù Par√¢metros do Firecrawl

### Par√¢metros de Scraping

```python
scraper.scrape(
    urls=None,              # Lista de URLs espec√≠ficas
    crawl=False,            # Se True, faz crawling do site
    max_pages=None,         # N√∫mero m√°ximo de p√°ginas (para crawl)
    formats=["markdown"],    # Formatos: markdown, html, text
    onlyMainContent=True,   # Apenas conte√∫do principal
    includeTags=["article"], # Tags HTML a incluir
)
```

### Par√¢metros de Crawling

```python
scraper.scrape(
    crawl=True,
    max_pages=50,
    limit=50,               # Limite de p√°ginas
    maxDepth=3,             # Profundidade m√°xima
    allowBackwardLinks=True, # Permitir links para tr√°s
)
```

---

## ‚ö†Ô∏è Considera√ß√µes Importantes

### 1. Rate Limiting
- Firecrawl gerencia rate limiting automaticamente
- Respeite os limites da sua conta
- Para grandes volumes, considere processar em lotes

### 2. Custos
- Firecrawl √© um servi√ßo pago (com tier gratuito limitado)
- Monitore o uso atrav√©s do dashboard
- Use `max_pages` para controlar custos

### 3. Qualidade dos Dados
- Firecrawl extrai conte√∫do limpo e estruturado
- Sempre valide os dados coletados
- Use `validate_data()` antes de adicionar ao RAG

### 4. Sites com JavaScript
- Firecrawl renderiza JavaScript automaticamente
- Ideal para SPAs (Single Page Applications)
- N√£o precisa de Selenium/Playwright

---

## üîç Debugging

### Ver Logs Detalhados

```python
import logging

logging.basicConfig(level=logging.DEBUG)
```

### Verificar Dados Coletados

```python
documents = scraper.scrape()

for doc in documents:
    print(f"T√≠tulo: {doc['title']}")
    print(f"URL: {doc['url']}")
    print(f"Tamanho: {len(doc['content'])} caracteres")
    print(f"Metadados: {doc['metadata']}")
    print("-" * 60)
```

---

## üìö Recursos Adicionais

- [Documenta√ß√£o Firecrawl](https://docs.firecrawl.dev/)
- [API Reference](https://docs.firecrawl.dev/api-reference)
- [Mapeamento de Fontes](./mapeamento_webscraping_edtech.md)

---

## üÜò Suporte

Para problemas ou d√∫vidas:
1. Verifique se a API key est√° configurada corretamente
2. Consulte os logs para erros espec√≠ficos
3. Teste com uma URL simples primeiro
4. Verifique a documenta√ß√£o do Firecrawl

---

**√öltima atualiza√ß√£o:** 2026-01-08
