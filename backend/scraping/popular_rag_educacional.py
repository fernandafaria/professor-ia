#!/usr/bin/env python3
"""
Script para popular RAG com conte√∫do educacional (BNCC, planos de aula, etc).
Usa o pipeline de scraping para coletar e adicionar conte√∫do educacional.
"""

import sys
import logging
from pathlib import Path

# Adicionar diret√≥rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Importar usando caminho relativo
from populate_rag import RAGPopulator

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """Fun√ß√£o principal."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Popular RAG com conte√∫do educacional"
    )
    parser.add_argument(
        "--phase",
        type=str,
        choices=["mvp", "cultural", "all"],
        default="mvp",
        help="Fase a executar: mvp (BNCC, √Ågatha, Nova Escola), cultural, ou all"
    )
    parser.add_argument(
        "--no-firecrawl",
        action="store_true",
        help="N√£o usar Firecrawl (usar scrapers tradicionais)"
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=50,
        help="N√∫mero m√°ximo de p√°ginas por fonte (padr√£o: 50)"
    )
    
    args = parser.parse_args()
    
    logger.info("=" * 60)
    logger.info("üöÄ Popular RAG com Conte√∫do Educacional")
    logger.info("=" * 60)
    logger.info("")
    
    # Verificar configura√ß√µes
    import os
    use_firecrawl = not args.no_firecrawl and os.getenv("FIRECRAWL_API_KEY")
    
    if not use_firecrawl:
        logger.warning("‚ö†Ô∏è  FIRECRAWL_API_KEY n√£o configurada - usando scrapers tradicionais")
    
    populator = RAGPopulator()
    
    try:
        if args.phase == "mvp":
            logger.info("üìö FASE MVP: Coletando conte√∫do educacional priorit√°rio")
            logger.info("   - API BNCC Cientificar (estrutura curricular)")
            logger.info("   - Projeto √Ågatha Edu (quest√µes ENEM/vestibulares)")
            logger.info("   - Nova Escola (planos de aula)")
            logger.info("")
            
            stats = populator.populate_phase1_mvp(use_firecrawl=use_firecrawl)
            
            logger.info("")
            logger.info("=" * 60)
            logger.info("üìä RESUMO - FASE MVP")
            logger.info("=" * 60)
            logger.info(f"Total de chunks coletados: {stats.get('total_chunks', 0)}")
            logger.info(f"Adicionado ao RAG: {'‚úÖ Sim' if stats.get('added_to_rag') else '‚ùå N√£o'}")
            
            if stats.get('sources'):
                logger.info("\nüìö Por fonte:")
                for source, source_stats in stats['sources'].items():
                    status = source_stats.get('status', 'unknown')
                    count = source_stats.get('documents', 0)
                    if status == 'success':
                        logger.info(f"   ‚úÖ {source}: {count} documentos")
                    else:
                        logger.info(f"   ‚ùå {source}: Erro - {source_stats.get('error', 'Desconhecido')}")
            
            if stats.get('errors'):
                logger.warning(f"\n‚ö†Ô∏è  {len(stats['errors'])} erros encontrados")
                for error in stats['errors'][:3]:  # Mostrar apenas primeiros 3
                    logger.warning(f"   - {error}")
        
        elif args.phase == "cultural":
            logger.info("üé® FASE CULTURAL: Coletando conte√∫do cultural")
            stats = populator.populate_cultural_content(max_pages_per_source=args.max_pages)
            
            logger.info("")
            logger.info("=" * 60)
            logger.info("üìä RESUMO - CONTE√öDO CULTURAL")
            logger.info("=" * 60)
            logger.info(f"Total de chunks: {stats.get('total_chunks', 0)}")
            logger.info(f"Adicionado ao RAG: {'‚úÖ Sim' if stats.get('added_to_rag') else '‚ùå N√£o'}")
        
        elif args.phase == "all":
            logger.info("üåê FASE COMPLETA: Coletando todo o conte√∫do")
            mvp_stats = populator.populate_phase1_mvp(use_firecrawl=use_firecrawl)
            cultural_stats = populator.populate_cultural_content(max_pages_per_source=args.max_pages)
            
            logger.info("")
            logger.info("=" * 60)
            logger.info("üìä RESUMO COMPLETO")
            logger.info("=" * 60)
            logger.info(f"MVP - Chunks: {mvp_stats.get('total_chunks', 0)}")
            logger.info(f"Cultural - Chunks: {cultural_stats.get('total_chunks', 0)}")
            total = mvp_stats.get('total_chunks', 0) + cultural_stats.get('total_chunks', 0)
            logger.info(f"Total: {total}")
        
        logger.info("")
        logger.info("‚úÖ Processo conclu√≠do!")
        logger.info("")
        logger.info("üß™ Verificar documentos no RAG:")
        logger.info("   python3 -c \"")
        logger.info("   from app.services.database import get_db")
        logger.info("   from sqlalchemy import text")
        logger.info("   db = next(get_db())")
        logger.info("   result = db.execute(text('SELECT source, COUNT(*) FROM rag_documents GROUP BY source'))")
        logger.info("   for row in result: print(f'{row[0]}: {row[1]} documentos')")
        logger.info("   \"")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Processo interrompido pelo usu√°rio")
        return 1
    except Exception as e:
        logger.error(f"‚ùå Erro durante execu√ß√£o: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
