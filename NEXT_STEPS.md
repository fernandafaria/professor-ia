# üöÄ Pr√≥ximos Passos - Plataforma P1A

**Data:** 2026-01-08  
**Status:** Sistema de Scraping Completo ‚úÖ  
**Pr√≥xima Fase:** Valida√ß√£o e Integra√ß√£o

---

## ‚úÖ O que j√° est√° pronto

1. ‚úÖ **Sistema de Scraping Completo**
   - Scrapers para todas as fontes do mapeamento
   - Processadores de conte√∫do
   - Pipeline integrado com RAG
   - Importador para dados BNCC j√° coletados

2. ‚úÖ **Estrutura Base**
   - FastAPI configurado
   - Sistema RAG (ChromaDB + Sentence Transformers)
   - Configura√ß√µes e schemas b√°sicos

3. ‚úÖ **Dados Dispon√≠veis**
   - Arquivo JSON com 1.617 itens BNCC (EF + EM)
   - Mapeamento completo de fontes

---

## üéØ Pr√≥ximos Passos Priorizados

### üî• FASE 1: Valida√ß√£o e Testes (Esta Semana)

#### 1.1 Testar Importa√ß√£o de Dados BNCC ‚ö° **URGENTE**

**Objetivo:** Validar que os dados coletados podem ser importados e indexados no RAG.

```bash
# 1. Verificar se ChromaDB est√° configurado
# 2. Testar importa√ß√£o
python -m backend.scraping.import_bncc_data "scraping/extract-data-2026-01-08 (1).json" --no-rag

# 3. Se funcionar, importar com RAG
python -m backend.scraping.import_bncc_data "scraping/extract-data-2026-01-08 (1).json"
```

**Checklist:**
- [ ] ChromaDB rodando e acess√≠vel
- [ ] Importa√ß√£o sem erros
- [ ] Documentos processados corretamente
- [ ] Metadados extra√≠dos (disciplina, s√©rie, etc.)
- [ ] Chunks criados adequadamente
- [ ] Dados adicionados ao ChromaDB

**Tempo estimado:** 2-3 horas

---

#### 1.2 Validar Sistema RAG ‚ö° **URGENTE**

**Objetivo:** Garantir que o RAG consegue buscar e recuperar documentos.

```python
# Criar script de teste
from app.core.rag.retriever import RAGRetriever

retriever = RAGRetriever()
results = retriever.retrieve(
    query="Como funciona a leitura de textos?",
    n_results=5,
    filters={"discipline": "L√≠ngua Portuguesa"}
)

print(f"Encontrados {len(results)} documentos")
for doc in results:
    print(f"- {doc['metadata'].get('title', 'Sem t√≠tulo')}")
```

**Checklist:**
- [ ] RAG consegue buscar documentos
- [ ] Embeddings funcionando corretamente
- [ ] Filtros por metadata funcionam
- [ ] Resultados s√£o relevantes
- [ ] Performance aceit√°vel (< 2s)

**Tempo estimado:** 2-3 horas

---

#### 1.3 Criar Script de Setup/Valida√ß√£o

**Objetivo:** Script que valida todo o ambiente antes de come√ßar.

```bash
# Criar: backend/scripts/validate_setup.py
python backend/scripts/validate_setup.py
```

**Valida√ß√µes:**
- [ ] PostgreSQL conectado
- [ ] ChromaDB acess√≠vel
- [ ] Redis rodando
- [ ] Vari√°veis de ambiente configuradas
- [ ] Depend√™ncias instaladas
- [ ] Modelos de embedding carregados

**Tempo estimado:** 1-2 horas

---

### üèóÔ∏è FASE 2: API e Integra√ß√£o (Pr√≥xima Semana)

#### 2.1 Criar Endpoints de RAG

**Objetivo:** API para fazer queries no sistema RAG.

**Endpoints necess√°rios:**
- `POST /api/v1/rag/query` - Buscar conte√∫do
- `GET /api/v1/rag/stats` - Estat√≠sticas da base
- `POST /api/v1/rag/add-documents` - Adicionar documentos manualmente

**Arquivo:** `backend/app/api/v1/routes/rag.py`

**Tempo estimado:** 4-6 horas

---

#### 2.2 Criar Endpoint de Scraping

**Objetivo:** API para executar scraping via HTTP.

**Endpoints:**
- `POST /api/v1/scraping/run` - Executar scraping de fonte
- `GET /api/v1/scraping/sources` - Listar fontes dispon√≠veis
- `GET /api/v1/scraping/status/{job_id}` - Status do job

**Arquivo:** `backend/app/api/v1/routes/scraping.py`

**Tempo estimado:** 3-4 horas

---

#### 2.3 Integrar com Celery (Background Jobs)

**Objetivo:** Executar scraping em background.

**Tarefas:**
- [ ] Configurar Celery workers
- [ ] Criar tasks para scraping
- [ ] Sistema de monitoramento de jobs
- [ ] Retry autom√°tico em caso de falha

**Tempo estimado:** 4-5 horas

---

### üß™ FASE 3: Testes e Qualidade (2 Semanas)

#### 3.1 Testes Unit√°rios

**Cobertura m√≠nima:**
- [ ] Testes dos scrapers
- [ ] Testes dos processadores
- [ ] Testes do RAG retriever
- [ ] Testes da pipeline

**Tempo estimado:** 6-8 horas

---

#### 3.2 Testes de Integra√ß√£o

**Cen√°rios:**
- [ ] Importa√ß√£o completa de dados BNCC
- [ ] Scraping de fonte real
- [ ] Query RAG end-to-end
- [ ] Performance e carga

**Tempo estimado:** 4-6 horas

---

### üìä FASE 4: Monitoramento e Observabilidade (2-3 Semanas)

#### 4.1 Logging Estruturado

**Objetivo:** Logs detalhados para debugging.

- [ ] Configurar structlog
- [ ] Logs de scraping
- [ ] Logs de RAG queries
- [ ] M√©tricas de performance

**Tempo estimado:** 3-4 horas

---

#### 4.2 M√©tricas e Dashboard

**Objetivo:** Monitorar sa√∫de do sistema.

- [ ] Prometheus metrics
- [ ] Dashboard b√°sico (Grafana ou similar)
- [ ] Alertas para falhas

**Tempo estimado:** 4-6 horas

---

### üé® FASE 5: Frontend e UX (3-4 Semanas)

#### 5.1 Interface de Administra√ß√£o

**Funcionalidades:**
- [ ] Dashboard de scraping
- [ ] Visualiza√ß√£o de documentos no RAG
- [ ] Teste de queries
- [ ] Gerenciamento de fontes

**Tempo estimado:** 2-3 semanas

---

## üìã Checklist R√°pido - Come√ßar Agora

### Setup Inicial (30 minutos)

```bash
# 1. Verificar ambiente
cd backend
python --version  # Deve ser 3.10+

# 2. Instalar depend√™ncias (se ainda n√£o fez)
pip install -r requirements.txt

# 3. Configurar .env
cp .env.example .env
# Editar .env com suas configura√ß√µes

# 4. Iniciar ChromaDB
chroma run --path ./chroma_db --port 8000

# 5. Iniciar Redis (se usar Celery)
redis-server
```

### Teste R√°pido (15 minutos)

```bash
# 1. Testar importa√ß√£o (sem RAG primeiro)
python -m backend.scraping.import_bncc_data \
  "scraping/extract-data-2026-01-08 (1).json" \
  --no-rag \
  --categories fundamental_education

# 2. Verificar se processou corretamente
# Deve mostrar estat√≠sticas de processamento
```

### Valida√ß√£o RAG (30 minutos)

```python
# Criar: backend/test_rag.py
from app.core.rag.retriever import RAGRetriever

retriever = RAGRetriever()

# Testar busca
results = retriever.retrieve("matem√°tica", n_results=3)
print(f"Encontrados: {len(results)}")
for r in results:
    print(f"- {r['metadata'].get('title')}")
```

---

## üéØ Prioridades por Urg√™ncia

### ‚ö° **URGENTE (Esta Semana)**
1. ‚úÖ Testar importa√ß√£o de dados BNCC
2. ‚úÖ Validar sistema RAG
3. ‚úÖ Criar script de valida√ß√£o de setup

### üî• **ALTA (Pr√≥xima Semana)**
4. ‚úÖ Criar endpoints de API
5. ‚úÖ Integrar scraping com API
6. ‚úÖ Testes b√°sicos

### üìà **M√âDIA (2-3 Semanas)**
7. ‚úÖ Celery para background jobs
8. ‚úÖ Testes completos
9. ‚úÖ Monitoramento b√°sico

### üí° **BAIXA (1 M√™s+)**
10. ‚úÖ Frontend admin
11. ‚úÖ Dashboard avan√ßado
12. ‚úÖ Otimiza√ß√µes de performance

---

## üõ†Ô∏è Comandos √öteis

### Desenvolvimento

```bash
# Rodar API
uvicorn app.main:app --reload

# Rodar scraping
python -m backend.scraping.cli --source "API BNCC Cientificar"

# Importar dados
python -m backend.scraping.import_bncc_data "scraping/extract-data-2026-01-08 (1).json"

# Testes
pytest backend/tests/
```

### Debugging

```bash
# Ver logs do ChromaDB
tail -f chroma_db/logs/*.log

# Verificar conex√£o Redis
redis-cli ping

# Verificar PostgreSQL
psql -U postgres -d p1a_db -c "SELECT 1"
```

---

## üìö Documenta√ß√£o de Refer√™ncia

- **Scraping:** `backend/scraping/README.md`
- **Importa√ß√£o:** `backend/scraping/IMPORT_GUIDE.md`
- **Arquitetura:** `ARCHITECTURE.md`
- **Setup:** `docs/DEVELOPMENT_SETUP.md`

---

## üÜò Troubleshooting

### Problema: ChromaDB n√£o conecta
```bash
# Verificar se est√° rodando
chroma run --path ./chroma_db --port 8000

# Verificar configura√ß√£o em app/config.py
CHROMA_HOST=localhost
CHROMA_PORT=8000
```

### Problema: Importa√ß√£o falha
```bash
# Testar sem RAG primeiro
python -m backend.scraping.import_bncc_data \
  "scraping/extract-data-2026-01-08 (1).json" \
  --no-rag

# Verificar logs
# Ajustar batch_size se necess√°rio
```

### Problema: RAG n√£o encontra documentos
```python
# Verificar se documentos foram adicionados
from app.core.rag.retriever import RAGRetriever
retriever = RAGRetriever()
collection = retriever.collection
print(f"Documentos na collection: {collection.count()}")
```

---

## ‚úÖ Pr√≥xima A√ß√£o Imediata

**Execute agora:**

```bash
# 1. Validar setup
cd backend
python -c "from app.config import settings; print('Config OK')"

# 2. Testar importa√ß√£o (sem RAG)
python -m backend.scraping.import_bncc_data \
  "../scraping/extract-data-2026-01-08 (1).json" \
  --no-rag \
  --categories fundamental_education

# 3. Se funcionar, importar com RAG
python -m backend.scraping.import_bncc_data \
  "../scraping/extract-data-2026-01-08 (1).json" \
  --categories fundamental_education
```

**Tempo estimado:** 30-60 minutos  
**Resultado esperado:** Dados BNCC importados e indexados no RAG

---

**√öltima Atualiza√ß√£o:** 2026-01-08  
**Pr√≥xima Revis√£o:** Ap√≥s completar Fase 1
